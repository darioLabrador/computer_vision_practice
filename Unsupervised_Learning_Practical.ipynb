{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darioLabrador/computer_vision_practice/blob/main/Unsupervised_Learning_Practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBUvnzEhq3i-"
      },
      "source": [
        "# Principal Components Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EwWA6j1fq3jC"
      },
      "source": [
        "- So far, we have been looking at supervised learning methods: they predict **labels** based on labelled training data.\n",
        "- We will now explore several **unsupervised** estimators, which can highlight interesting aspects of the data without reference to any known labels.\n",
        "- We will begin with one of the most used unsupervised algorithms, **Principal Components Analysis (PCA)**.\n",
        "- PCA is a dimensionality-reduction algorithm, but it can also be useful as a tool for visualisation, noise filtering, feature extraction, engineering, and much more.\n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2HZFMmz_q3jD"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import (\n",
        "    AgglomerativeClustering,\n",
        "    DBSCAN,\n",
        "    KMeans,\n",
        ")\n",
        "from sklearn.datasets import (\n",
        "    fetch_lfw_people,\n",
        "    load_diabetes,\n",
        "    load_iris,\n",
        "    make_blobs,\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "rng = np.random.default_rng(1)\n",
        "sns.set(style='white')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "I74LExaUq3jE"
      },
      "source": [
        "## Introducing Principal Components Analysis\n",
        "\n",
        "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data.\n",
        "\n",
        "Its behavior is easiest to visualize by looking at a two-dimensional dataset.\n",
        "Consider these 200 points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FSQY1poAq3jE"
      },
      "outputs": [],
      "source": [
        "X = np.dot(rng.uniform(size=(2, 2)), rng.normal(size=(2, 200))).T\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], alpha=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "AxHmkhfZq3jG"
      },
      "source": [
        "By eye, it is clear that there is a nearly linear relationship between the *x* and *y* variables.\n",
        "This is reminiscent of linear regression, but the problem setting here is slightly different: rather than attempting to *predict* the *y* values from the *x* values, the unsupervised learning problem attempts to learn about the *relationship* between the *x* and *y* values.\n",
        "\n",
        "In principal components analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset.\n",
        "Using Scikit-Learn's `PCA` estimator, we can compute this as follows.\n",
        "\n",
        "The fit learns some quantities from the data, most importantly the components and explained variance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "L-eq9Xnnq3jG"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "print('Loadings:\\n', pca.components_)\n",
        "print('\\nExplained Variance Ratio:\\n', pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "59xqGIF0q3jK"
      },
      "source": [
        "To see what these numbers mean, let's visualise them as vectors over the input data, using the components to define the direction of the vector and the explained variance to define the squared length of the vector (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "DBnLmNrdq3jK"
      },
      "outputs": [],
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops = {\n",
        "        'arrowstyle': '->', 'linewidth': 1, 'color': 'black',\n",
        "        'shrinkA': 0, 'shrinkB': 0,\n",
        "    }\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "fig.subplots_adjust(wspace=0.3)\n",
        "\n",
        "# plot data\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], alpha=0.2, ax=ax[0])\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
        "# ax[0].axis('equal')\n",
        "ax[0].set(xlabel='X', ylabel='Y', title='Original Data')\n",
        "\n",
        "# plot principal components\n",
        "X_pca = pca.transform(X)\n",
        "ev = pca.explained_variance_\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], alpha=0.2, ax=ax[1])\n",
        "draw_vector([0, 0], [3*np.sqrt(ev[0]), 0], ax=ax[1])\n",
        "draw_vector([0, 0], [0, 3*np.sqrt(ev[1])], ax=ax[1])\n",
        "ax[1].set(\n",
        "    xlabel='PC1', ylabel='PC2',\n",
        "    title='Principal Components',\n",
        "    xlim=(-5, 5), ylim=(-3, 3.1)\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "22OgOT91q3jL"
      },
      "source": [
        "These vectors represent the principal axes of the data, and the length of each vector is an indication of how \"important\" that axis is in describing the distribution of the dataâ€”more precisely, it is a measure of the variance of the data when projected onto that axis.\n",
        "The projection of each data point onto the principal axes are the principal components of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1Qa7Hk8rq3jN"
      },
      "source": [
        "### PCA for Dimensionality Reduction\n",
        "\n",
        "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
        "\n",
        "Here is an example of using PCA as a dimensionality reduction transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JGG8Vym1q3jN"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "print(f'Original Shape: {X.shape}')\n",
        "print(f'Transformed Shape: {X_pca.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=X_pca, y=[0]*200);"
      ],
      "metadata": {
        "id": "4pKEUV4Dp1fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qyHSPZ7wq3jN"
      },
      "source": [
        "The transformed data has been reduced to a single dimension.\n",
        "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lzjMAQSWq3jN"
      },
      "outputs": [],
      "source": [
        "X_new = pca.inverse_transform(X_pca)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BpA6fFvBq3jO"
      },
      "source": [
        "The blue points are the original data, while the orange points are the projected version.\n",
        "This clarifies what a PCA is doing: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n",
        "The fraction of variance that is cut out (proportional to the spread of points about the line formed in the preceding figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
        "\n",
        "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the number of data features by 50%, the overall relationships between the data points are mostly preserved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Biplots: Adding Interpretability to Principal Components\n",
        "\n",
        "Although PCA causes a loss in interpretability due to PCs being linear combinations of the original features, it is possible to retain *some* understanding on the variability of data with respect to each feature.\n",
        "\n",
        "This is achieved through **biplots**, which present the transformed data, together with an overlay of the *loadings* vectors.\n",
        "\n",
        "Lets first load and transform the `iris` data set:"
      ],
      "metadata": {
        "id": "_Du3eFt0rEMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "nu = len(np.unique(y))\n",
        "pal = sns.color_palette('Set2', nu)\n",
        "feats = [feat[:-5] for feat in iris['feature_names']]\n",
        "y_names = pd.Series(y, name='Species').map({\n",
        "    0: 'Setosa', 1: 'Versicolor', 2: 'Virginica',\n",
        "})\n",
        "\n",
        "# Scale the data prior to PCA\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "# Transform the data\n",
        "pca = PCA()\n",
        "X_pc = pca.fit_transform(X)\n",
        "\n",
        "print(f'Explained Variance Ratios: {pca.explained_variance_ratio_[:2]}')"
      ],
      "metadata": {
        "id": "As5N2784rLPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As there is no built-in function for **biplots** in `sklearn`, we will need to define our own:"
      ],
      "metadata": {
        "id": "FYUfMjLWB-jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def biplot(X_pc, coeff, feats):\n",
        "    xs = X_pc[:, 0]\n",
        "    ys = X_pc[:, 1]\n",
        "    n_feats = coeff.shape[0]\n",
        "    pca2load, load2pca = lambda x: x/3, lambda x: x*3\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "    sns.scatterplot(x=xs, y=ys, hue=y_names, palette=pal)\n",
        "    for i in range(n_feats):\n",
        "        ax.arrow(\n",
        "            0, 0, 3*coeff[i,0], 3*coeff[i,1], color='r',\n",
        "            head_width=0.1, head_length=0.2, length_includes_head=True,\n",
        "        )\n",
        "        ax.text(\n",
        "            3.1*coeff[i, 0], 3.1*coeff[i, 1], feats[i],\n",
        "            color='r', ha='left', va='center',\n",
        "        )\n",
        "    ax.set(xlabel='PC1', ylabel='PC2')\n",
        "    xsecax = ax.secondary_xaxis('top', functions=(pca2load, load2pca))\n",
        "    ysecax = ax.secondary_yaxis('right', functions=(pca2load, load2pca))\n",
        "    xsecax.set_xticks([-0.5, 0, 0.5])\n",
        "    ysecax.set_yticks([-0.5, 0, 0.5])\n",
        "    xsecax.set_xlabel('PC1 Loadings')\n",
        "    ysecax.set_ylabel('PC2 Loadings', rotation=-90, va='bottom')\n",
        "    ax.grid();\n",
        "\n",
        "# Call the function. Use only the first 2 PCs.\n",
        "biplot(X_pc[:, 0:2], pca.components_[0:2, :].T, feats)"
      ],
      "metadata": {
        "id": "8jzFIoSfB_XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this plot we can see that:\n",
        "- *Setosa* is smaller in `petal length`, `petal width`, and `sepal length` than the other two species\n",
        "- *Virginica* is slightly higher for the same three features than the other two species\n",
        "- `sepal width` distribution is high on PC2, but low on PC1, meaning that it is **not as relevant** in explaining the variance of the dataset. This can be confirmed by the **Explained Variance Ratios** for PC1 and PC2 (approx. 0.73 and 0.23, respectively)"
      ],
      "metadata": {
        "id": "ajRv4jmX_hUD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QB8LLhqUq3jY"
      },
      "source": [
        "## Example: Eigenfaces\n",
        "\n",
        "We will be using the *Labeled Faces in the Wild* (LFW) dataset made available through Scikit-Learn to see how PCA may be used to reduce the dimensionality of data, yet be good enough to preserve information.\n",
        "\n",
        "Lets first import the LFW data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "16p0tXptq3jY"
      },
      "outputs": [],
      "source": [
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)\n",
        "print(f'Pixels per image: {62*47}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "T40JFqzrq3jZ"
      },
      "source": [
        "- Let's take a look at the principal axes that span this dataset.\n",
        "- Because this is a large dataset, we will use the `'randomized'` eigensolver in `PCA`: it uses a randomised method to approximate the first $N$ principal components more quickly than the standard approach, at the expense of some accuracy.\n",
        "- This trade-off can be useful for high-dimensional data (in this case, nearly $3,000$ pixels per image).\n",
        "- We will consider the first $150$ components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-p1fzliXq3jZ"
      },
      "outputs": [],
      "source": [
        "pca = PCA(150, svd_solver='randomized', random_state=42)\n",
        "pca.fit(faces.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "l9CdUl3oq3jZ"
      },
      "source": [
        "In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as *eigenvectors*,\n",
        "so these types of images are often called *eigenfaces*; as you can see in the following figure, they are as creepy as they sound):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "i_F5xPvQq3ja"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw={'hspace':0.1, 'wspace':0.1})\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZLW9uHaCq3ja"
      },
      "source": [
        "The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips.\n",
        "Let's take a look at the cumulative variance of these components to see how much of the data information the projection is preserving (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "NKJCiP9Cq3ja"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kF05ogqEq3ja"
      },
      "source": [
        "The 150 components we have chosen account for just over 90% of the variance.\n",
        "That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data.\n",
        "To make this more concrete, we can compare the input images with the images reconstructed from these 150 components (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ZACi7xbFq3jb"
      },
      "outputs": [],
      "source": [
        "# Compute the components and projected faces\n",
        "pca = pca.fit(faces.data)\n",
        "components = pca.transform(faces.data)\n",
        "projected = pca.inverse_transform(components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "uT9y25Goq3jb"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n",
        "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "for i in range(10):\n",
        "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
        "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
        "\n",
        "ax[0, 0].set_ylabel('Full-dim\\nInput')\n",
        "ax[1, 0].set_ylabel('150-dim\\nReconstruct');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "j64zUUtnq3jb"
      },
      "source": [
        "The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features.\n",
        "Although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in each image. This means a classification algorithm only needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which, depending on the particular algorithm we choose, can lead to much more efficient classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hands On\n",
        "\n",
        "Try PCA on the **Diabetes** dataset, which may be loaded using sklearn's `load_diabetes` function. Generate a biplot for the first two components, and give an interpretation of what you see."
      ],
      "metadata": {
        "id": "QkYsLL0ZDuVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "diabetes = load_diabetes()"
      ],
      "metadata": {
        "id": "YAuyXTCBEnWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "papermill": {
          "duration": 0.02275,
          "end_time": "2021-07-11T12:36:39.102024",
          "exception": false,
          "start_time": "2021-07-11T12:36:39.079274",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "95f104f6"
      },
      "source": [
        "# K-Means Clustering\n",
        "\n",
        "Let's create our own dataset for this section\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "execution": {
          "iopub.execute_input": "2021-07-11T12:36:39.302251Z",
          "iopub.status.busy": "2021-07-11T12:36:39.301632Z",
          "iopub.status.idle": "2021-07-11T12:36:39.308230Z",
          "shell.execute_reply": "2021-07-11T12:36:39.307733Z",
          "shell.execute_reply.started": "2021-07-11T12:15:50.822416Z"
        },
        "new_sheet": false,
        "papermill": {
          "duration": 0.034715,
          "end_time": "2021-07-11T12:36:39.308373",
          "exception": false,
          "start_time": "2021-07-11T12:36:39.273658",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "2e5e17ac"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(\n",
        "    n_samples=5000,\n",
        "    centers=[[4,4], [-2, -1], [2, -3], [1, 1]],\n",
        "    cluster_std=0.9,\n",
        ")\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], marker='.');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "papermill": {
          "duration": 0.03699,
          "end_time": "2021-07-11T12:36:39.670040",
          "exception": false,
          "start_time": "2021-07-11T12:36:39.633050",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "d4462b32"
      },
      "source": [
        "<h2 id=\"setting_up_K_means\">Setting up K-Means</h2>\n",
        "Now that we have our random data, let's set up our K-Means Clustering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "papermill": {
          "duration": 0.031262,
          "end_time": "2021-07-11T12:36:39.741030",
          "exception": false,
          "start_time": "2021-07-11T12:36:39.709768",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "d2385eb8"
      },
      "source": [
        "The `KMeans` class has many parameters that can be used, but we will be using these two:\n",
        "\n",
        "- **n_clusters**: The number of clusters to form as well as the number of centroids to generate. We will set this to $4$ (as we have 4 centers)\n",
        "- **n_init**: Number of time the K-Means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. We will set this to $12$.\n",
        "\n",
        "Initialize KMeans with these parameters, where the output parameter is called `k_means`, and fit the model to our data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "execution": {
          "iopub.execute_input": "2021-07-11T12:36:39.802424Z",
          "iopub.status.busy": "2021-07-11T12:36:39.801777Z",
          "iopub.status.idle": "2021-07-11T12:36:39.805490Z",
          "shell.execute_reply": "2021-07-11T12:36:39.806126Z",
          "shell.execute_reply.started": "2021-07-11T12:18:15.573293Z"
        },
        "new_sheet": false,
        "papermill": {
          "duration": 0.032954,
          "end_time": "2021-07-11T12:36:39.806302",
          "exception": false,
          "start_time": "2021-07-11T12:36:39.773348",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "f480f7c0"
      },
      "outputs": [],
      "source": [
        "k_means = KMeans(n_clusters=4, n_init=12)\n",
        "k_means.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "papermill": {
          "duration": 0.03153,
          "end_time": "2021-07-11T12:36:41.541531",
          "exception": false,
          "start_time": "2021-07-11T12:36:41.510001",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "bb87ad34"
      },
      "source": [
        "We can get the labels for each point in the model using K-Means' `labels_` attribute:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "execution": {
          "iopub.execute_input": "2021-07-11T12:36:41.612750Z",
          "iopub.status.busy": "2021-07-11T12:36:41.612052Z",
          "iopub.status.idle": "2021-07-11T12:36:41.616348Z",
          "shell.execute_reply": "2021-07-11T12:36:41.616798Z",
          "shell.execute_reply.started": "2021-07-11T12:18:29.422786Z"
        },
        "new_sheet": false,
        "papermill": {
          "duration": 0.042977,
          "end_time": "2021-07-11T12:36:41.616983",
          "exception": false,
          "start_time": "2021-07-11T12:36:41.574006",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "551d3448"
      },
      "outputs": [],
      "source": [
        "k_means.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "papermill": {
          "duration": 0.032689,
          "end_time": "2021-07-11T12:36:41.681813",
          "exception": false,
          "start_time": "2021-07-11T12:36:41.649124",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "4f5d4dcc"
      },
      "source": [
        "We will also get the coordinates of the cluster centers using K-Means' `cluster_centers_` attribute:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "execution": {
          "iopub.execute_input": "2021-07-11T12:36:41.754916Z",
          "iopub.status.busy": "2021-07-11T12:36:41.754147Z",
          "iopub.status.idle": "2021-07-11T12:36:41.757172Z",
          "shell.execute_reply": "2021-07-11T12:36:41.755490Z",
          "shell.execute_reply.started": "2021-07-11T12:18:38.101534Z"
        },
        "new_sheet": false,
        "papermill": {
          "duration": 0.043258,
          "end_time": "2021-07-11T12:36:41.757325",
          "exception": false,
          "start_time": "2021-07-11T12:36:41.714067",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "9f1eb80d"
      },
      "outputs": [],
      "source": [
        "k_means.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "papermill": {
          "duration": 0.032843,
          "end_time": "2021-07-11T12:36:41.822507",
          "exception": false,
          "start_time": "2021-07-11T12:36:41.789664",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "c01f310c"
      },
      "source": [
        "Now that we have the random data generated and the K-Means model initialised, let's plot them and see what it looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "execution": {
          "iopub.execute_input": "2021-07-11T12:36:41.946235Z",
          "iopub.status.busy": "2021-07-11T12:36:41.945576Z",
          "iopub.status.idle": "2021-07-11T12:36:42.050766Z",
          "shell.execute_reply": "2021-07-11T12:36:42.051235Z",
          "shell.execute_reply.started": "2021-07-11T12:19:53.281003Z"
        },
        "new_sheet": false,
        "papermill": {
          "duration": 0.139136,
          "end_time": "2021-07-11T12:36:42.051423",
          "exception": false,
          "start_time": "2021-07-11T12:36:41.912287",
          "status": "completed"
        },
        "run_control": {
          "read_only": false
        },
        "tags": [],
        "id": "283ec4d4"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 4))\n",
        "colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means.labels_))))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "for k, col in enumerate(colors):\n",
        "    clus = (k_means.labels_ == k)\n",
        "    cluster_center = k_means.cluster_centers_[k]\n",
        "    ax.plot(X[clus, 0], X[clus, 1], 'w', markerfacecolor=col, marker='.')\n",
        "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
        "            markeredgecolor='k', markersize=6)\n",
        "\n",
        "ax.set_title('K-Means');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.026535,
          "end_time": "2021-07-11T12:36:42.105099",
          "exception": false,
          "start_time": "2021-07-11T12:36:42.078564",
          "status": "completed"
        },
        "tags": [],
        "id": "d39080c3"
      },
      "source": [
        "## Hands On\n",
        "\n",
        "Try to cluster the above dataset into 3 clusters.\n",
        "\n",
        "**Do not generate the data again**, use the same dataset as above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-11T12:36:42.167006Z",
          "iopub.status.busy": "2021-07-11T12:36:42.166329Z",
          "iopub.status.idle": "2021-07-11T12:36:43.993175Z",
          "shell.execute_reply": "2021-07-11T12:36:43.993722Z",
          "shell.execute_reply.started": "2021-07-11T12:21:29.165503Z"
        },
        "papermill": {
          "duration": 1.861396,
          "end_time": "2021-07-11T12:36:43.993906",
          "exception": false,
          "start_time": "2021-07-11T12:36:42.132510",
          "status": "completed"
        },
        "tags": [],
        "id": "8431e0b5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DBSCAN\n",
        "\n",
        "- We will now use DBSCAN, and compare it with K-Means and hierarchical clustering.\n",
        "- Let's create a concentric circles data set, and see how well the different methods are able to find the right clusters:"
      ],
      "metadata": {
        "id": "5xAZSU-LV6hg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M637PxO_myqI"
      },
      "source": [
        "def donut(r, n=100):\n",
        "    return np.stack((\n",
        "        np.cos(np.linspace(0, 2*np.pi, n, endpoint=False))*r,\n",
        "        np.sin(np.linspace(0, 2*np.pi, n, endpoint=False))*r,\n",
        "    ), axis=1) + rng.normal(-30, 30, size=(n, 2))\n",
        "\n",
        "df1 = pd.DataFrame(donut(500, 1500))\n",
        "df2 = pd.DataFrame(donut(300, 1000))\n",
        "df3 = pd.DataFrame(donut(100, 500))\n",
        "dfn = pd.DataFrame(rng.uniform(-650, 650, (250, 2)))\n",
        "df = pd.concat([df1, df2, df3, dfn]).rename(columns={0: 'x1', 1: 'x2'})\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEimH2rhnjYH"
      },
      "source": [
        "sns.relplot(data=df, x='x1', y='x2');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means vs Hierarchical vs DBSCAN\n",
        "\n",
        "### K-Means"
      ],
      "metadata": {
        "id": "tX5NfHf1s3TB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y12DnziOoAz0"
      },
      "source": [
        "k_means=KMeans(n_clusters=3)\n",
        "k_means.fit(df[['x1', 'x2']])\n",
        "df['K Means'] = k_means.labels_\n",
        "df['K Means'] = df['K Means'].astype('category')\n",
        "\n",
        "sns.relplot(data=df, x='x1', y='x2', hue='K Means');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical Clustering"
      ],
      "metadata": {
        "id": "MUgcGSb6s_u0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SByh2-C3omaa"
      },
      "source": [
        "hier = AgglomerativeClustering(n_clusters=3, metric='euclidean')\n",
        "hier.fit(df[['x1', 'x2']])\n",
        "df['Hierarchical'] = hier.labels_\n",
        "df['Hierarchical'] = df['Hierarchical'].astype('category')\n",
        "\n",
        "sns.relplot(data=df, x='x1', y='x2', hue='Hierarchical');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN without tuning\n",
        "\n",
        "Let's run DBSCAN on its default parameters: $É› = 0.5$ and `min_samples = 5`"
      ],
      "metadata": {
        "id": "rB6zALV2T6EH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSq0UbIbpBrN"
      },
      "source": [
        "dbscan = DBSCAN()\n",
        "dbscan.fit(df[['x1', 'x2']])\n",
        "df['DBSCAN Untuned'] = dbscan.labels_\n",
        "df['DBSCAN Untuned'] = df['DBSCAN Untuned'].astype('category')\n",
        "\n",
        "sns.relplot(data=df, x='x1', y='x2', hue='DBSCAN Untuned');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN does not seem to be working, as it is detecting just one cluster.\n",
        "\n",
        "**This is happening because the default $É›$ is too small**.\n",
        "\n",
        "The parameter `min_samples` is usually tuned using domain knowledge. For this example, `min_samples=6` works well.\n",
        "\n",
        "To estimate $É›$, we need to plot the $(min\\_samples - 1)$-distance graph, and find the **elbow** value:"
      ],
      "metadata": {
        "id": "HqdnnDQUH6XD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkDQCbEppczK"
      },
      "source": [
        "min_samples = 6\n",
        "neigh = NearestNeighbors(n_neighbors=min_samples-1)\n",
        "nbrs = neigh.fit(df[['x1', 'x2']])\n",
        "distances, indices = nbrs.kneighbors(df[['x1', 'x2']])\n",
        "\n",
        "# Plot the K-distance Graph\n",
        "distances = np.sort(distances, axis=0)[::-1]\n",
        "distances = distances[:, 1]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(distances, label=f'{min_samples - 1}-Distance')\n",
        "plt.axhline(y=30, color='r', linestyle='--', label=r'Elbow: $\\varepsilon = 30$')\n",
        "plt.xlabel('Sorted Points')\n",
        "plt.ylabel(r'$\\varepsilon$')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimum $É›$ is found at the *elbow* in the K-Distance Graph, $É› = 30$\n",
        "\n",
        "### Tuned DBSCAN"
      ],
      "metadata": {
        "id": "TgqGgHLqS0PJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Z6PtcEUjKz"
      },
      "source": [
        "dbscan_opt = DBSCAN(eps=30, min_samples=6)\n",
        "dbscan_opt.fit(df[['x1', 'x2']])\n",
        "df['DBSCAN Tuned'] = dbscan_opt.labels_\n",
        "df['DBSCAN Tuned'] = df['DBSCAN Tuned'].astype('category')\n",
        "\n",
        "sns.relplot(data=df, x='x1', y='x2', hue='DBSCAN Tuned');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands On\n",
        "\n",
        "Try the different clustering methods on the `iris` dataset, and tune the necessary parameters to try and detect each species."
      ],
      "metadata": {
        "id": "4t_t7-lKGAUD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMHd-IeAgGRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('3.9.6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "513788764cd0ec0f97313d5418a13e1ea666d16d72f976a8acadce25a5af2ffc"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}